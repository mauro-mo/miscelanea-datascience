{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos:\n",
      "(891, 12)\n",
      "(418, 11)\n",
      "Tipos de datos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n",
      "Datos faltantes:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "Estadísticas del dataset:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
      "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
      "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
      "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
      "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
      "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
      "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
      "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
      "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200\n"
     ]
    }
   ],
   "source": [
    "#Verifico la cantidad de datos que hay en los dataset\n",
    "print('Cantidad de datos:')\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "#Verifico el tipo de datos contenida en ambos dataset\n",
    "print('Tipos de datos:')\n",
    "print(df_train.info())\n",
    "print(df_test.info())\n",
    "\n",
    "#Verifico los datos faltantes de los dataset\n",
    "print('Datos faltantes:')\n",
    "print(pd.isnull(df_train).sum())\n",
    "print(pd.isnull(df_test).sum())\n",
    "\n",
    "#Verifico las estadísticas del dataset\n",
    "print('Estadísticas del dataset:')\n",
    "print(df_train.describe())\n",
    "print(df_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio los datos de sexos en números en train\n",
    "\n",
    "map_dict = {'female': 0, 'male': 1}\n",
    "df_train = df_train.replace({'Sex': map_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cambio los datos de sexos en números en test\n",
    "\n",
    "map_dict = {'female': 0, 'male': 1}\n",
    "df_test = df_test.replace({'Sex': map_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio los datos de embarque en números\n",
    "df_train['Embarked'].replace(['Q','S', 'C'],[0,1,2],inplace=True)\n",
    "df_test['Embarked'].replace(['Q','S', 'C'],[0,1,2],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69911764705882\n",
      "30.272590361445783\n"
     ]
    }
   ],
   "source": [
    "#Reemplazo los datos faltantes en la edad por la media de esta columna\n",
    "\n",
    "print(df_train[\"Age\"].mean())\n",
    "print(df_test[\"Age\"].mean())\n",
    "promedio = 30\n",
    "df_train['Age'] = df_train['Age'].replace(np.nan, promedio)\n",
    "df_test['Age'] = df_test['Age'].replace(np.nan, promedio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo varios grupos de acuerdo a bandas de las edades\n",
    "#Bandas: 0-8, 9-15, 16-18, 19-25, 26-40, 41-60, 61-100\n",
    "\n",
    "bins = [0, 8, 15, 18, 25, 40, 60, 100]\n",
    "names = ['1', '2', '3', '4', '5', '6', '7']\n",
    "df_train['Age'] = pd.cut(df_train['Age'], bins, labels = names)\n",
    "df_test['Age'] = pd.cut(df_test['Age'], bins, labels = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "(889, 8)\n",
      "(417, 8)\n",
      "   PassengerId  Pclass  Sex Age  SibSp  Parch     Fare  Embarked\n",
      "0          892       3    1   5      0      0   7.8292         0\n",
      "1          893       3    0   6      1      0   7.0000         1\n",
      "2          894       2    1   7      0      0   9.6875         0\n",
      "3          895       3    1   5      0      0   8.6625         1\n",
      "4          896       3    0   4      1      1  12.2875         1\n",
      "   Survived  Pclass  Sex Age  SibSp  Parch     Fare  Embarked\n",
      "0         0       3    1   4      1      0   7.2500       1.0\n",
      "1         1       1    0   5      1      0  71.2833       2.0\n",
      "2         1       3    0   5      0      0   7.9250       1.0\n",
      "3         1       1    0   5      1      0  53.1000       1.0\n",
      "4         0       3    1   5      0      0   8.0500       1.0\n"
     ]
    }
   ],
   "source": [
    "#Se elimina la columna de \"Cabin\" ya que tiene muchos datos perdidos\n",
    "\n",
    "df_train.drop(['Cabin'], axis = 1, inplace=True)\n",
    "df_test.drop(['Cabin'], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "#Elimino las columnas que considero que no son necesarias para el analisis\n",
    "\n",
    "df_train = df_train.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "df_test = df_test.drop(['Name','Ticket'], axis=1)\n",
    "\n",
    "\n",
    "#Se elimina las filas con los datos perdidos\n",
    "\n",
    "df_train.dropna(axis=0, how='any', inplace=True)\n",
    "df_test.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "\n",
    "#Verifico los datos\n",
    "\n",
    "print(pd.isnull(df_train).sum())\n",
    "print(pd.isnull(df_test).sum())\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "print(df_test.head())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora si empezamos a implementar los algoritmos de Machine Learning, para este proyecto vamos a implementar algoritmos de regresión logística, vectores de soporte y vecinos más cercanos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separo la columna con la información de los sobrevivientes\n",
    "X = np.array(df_train.drop(['Survived'], axis= 1))\n",
    "y = np.array(df_train['Survived'])\n",
    "\n",
    "\n",
    "#Separo los datos de \"train\" en entrenamiento y prueba para probar los algoritmos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Regresión Logística:\n",
      "0.7974683544303798\n",
      "Precisión Soporte de Vectores:\n",
      "0.8593530239099859\n",
      "Precisión Vecinos más Cercanos:\n",
      "0.8621659634317862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Usuario\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "##Regresión logística\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "print('Precisión Regresión Logística:')\n",
    "print(logreg.score(X_train, y_train))\n",
    "\n",
    "\n",
    "##Support Vector Machines\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "print('Precisión Soporte de Vectores:')\n",
    "print(svc.score(X_train, y_train))\n",
    "\n",
    "\n",
    "##K neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "print('Precisión Vecinos más Cercanos:')\n",
    "print(knn.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya teniendo nuestros modelos procedemos a realizar la predicción respectiva utilizando la data de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción Regresión Logística:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         1\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n",
      "Predicción Soporte de Vectores:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         1\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n",
      "Predicción Vecinos más Cercanos:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n"
     ]
    }
   ],
   "source": [
    "ids = df_test['PassengerId']\n",
    "\n",
    "# 0: no sobrevive. 1: si sobrevive.\n",
    "\n",
    "##Regresión logística\n",
    "prediccion_logreg = logreg.predict(df_test.drop('PassengerId', axis=1))\n",
    "out_logreg = pd.DataFrame({ 'PassengerId' : ids, 'Survived': prediccion_logreg })\n",
    "print('Predicción Regresión Logística:')\n",
    "print(out_logreg.head())\n",
    "\n",
    "##Support Vector Machines\n",
    "prediccion_svc = svc.predict(df_test.drop('PassengerId', axis=1))\n",
    "out_svc = pd.DataFrame({ 'PassengerId' : ids, 'Survived': prediccion_svc })\n",
    "print('Predicción Soporte de Vectores:')\n",
    "print(out_svc.head())\n",
    "\n",
    "##K neighbors\n",
    "prediccion_knn = knn.predict(df_test.drop('PassengerId', axis=1))\n",
    "out_knn = pd.DataFrame({ 'PassengerId' : ids, 'Survived': prediccion_knn })\n",
    "print('Predicción Vecinos más Cercanos:')\n",
    "print(out_knn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjate que las predicciones del primer y último algoritmo son muy parecidas, así también lo fue los resultados de precisión, mientras que el segundo algoritmo que tuvo una precisión mucho mayor la predicción es un poco diferente por lo que esta sería más confiable que la de los otros."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
